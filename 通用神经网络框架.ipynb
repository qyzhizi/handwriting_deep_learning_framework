{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    我们把这个Node类作为这个神经网络的基础模块\n",
    "    \"\"\"\n",
    "    def __init__(self,inputs=[],name=None,is_trainable=False):\n",
    "        \n",
    "        self.inputs = inputs  #这个节点的输入，输入的是Node组成的列表\n",
    "        self.outputs = []     #这个节点的输出节点\n",
    "        self.name = name\n",
    "        self.is_trainable = is_trainable\n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)  #这个节点正好对应了这个输人的输出节点，从而建立了连接关系\n",
    "            \n",
    "        self.value = None  #每个节点必定对应有一个值\n",
    "        self.gradients = {}  #每个节点对上个节点的梯度，\n",
    "                             \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        先预留一个方法接口不实现，在其子类中实现,且要求其子类一定要实现，不实现的时话会报错。\n",
    "        \"\"\"\n",
    "        raise NotImplemented  \n",
    "        \n",
    "    def backward(self):\n",
    "        \n",
    "        raise NotImplemented\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return \"Node:{}\".format(self.name)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Placeholder(Node):\n",
    "    \"\"\"\n",
    "    作为x,k,b,weights和bias这类需要赋初始值和更新值的类\n",
    "    \"\"\"\n",
    "    def __init__(self,name=None,is_trainable=True):\n",
    "        \n",
    "        Node.__init__(self,name=name,is_trainable=is_trainable)\n",
    "        \n",
    "        \n",
    "    def forward(self,value=None):\n",
    "        \n",
    "        if value is not None: self.value = value\n",
    "        \n",
    "    def backward(self):\n",
    "        \n",
    "        self.gradients[self] = np.zeros_like(self.value).reshape((self.value.shape[0],-1))\n",
    "        \n",
    "        for n in self.outputs:\n",
    "           \n",
    "            self.gradients[self] += n.gradients[self].reshape((n.gradients[self].shape[0],-1))  #没有输入。\n",
    "            \n",
    "class Linear(Node):\n",
    "    \n",
    "    def __init__(self,x=None,weight=None,bias=None,name=None,is_trainable=False):\n",
    "        \n",
    "        Node.__init__(self,[x,weight,bias],name=name,is_trainable=is_trainable)\n",
    "        \n",
    "    \n",
    "    def forward(self):\n",
    "        \n",
    "        x, k, b =self.inputs[0], self.inputs[1], self.inputs[2]\n",
    "        \n",
    "        self.value = np.dot(x.value,k.value) + b.value.squeeze()\n",
    "      \n",
    "    def backward(self):\n",
    "        x, k, b =self.inputs[0], self.inputs[1], self.inputs[2]\n",
    "         \n",
    "        self.gradients[k] = np.zeros_like(k.value)\n",
    "        self.gradients[b] = np.zeros_like(b.value).reshape((len(np.zeros_like(b.value))))\n",
    "        self.gradients[x] = np.zeros_like(x.value)\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            \n",
    "            gradients_from_loss_to_self = n.gradients[self] \n",
    "            self.gradients[k] += np.dot(gradients_from_loss_to_self.T,x.value).T\n",
    "            self.gradients[b] += np.mean(gradients_from_loss_to_self,axis=0,keepdims=False).reshape((len(np.zeros_like(b.value))))\n",
    "            self.gradients[x] += np.dot(gradients_from_loss_to_self,k.value.T)\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \n",
    "    def __init__(self,x,name=None,is_trainable=False):\n",
    "        \n",
    "        Node.__init__(self,[x],name=name,is_trainable=is_trainable)\n",
    "        self.x = self.inputs[0]\n",
    "    \n",
    "    def _Sigmoid(self,x):\n",
    "        \n",
    "        return 1. /(1+np.exp(-1*x))\n",
    "    \n",
    "    def forward(self):\n",
    "        \n",
    "        self.value = self._Sigmoid(self.x.value)\n",
    "        \n",
    "    def partial(self):\n",
    "        \n",
    "        return self._Sigmoid(self.x.value) * (1 - self._Sigmoid(self.x.value))\n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        self.gradients[self.x] = np.zeros_like(self.value)\n",
    "     \n",
    "        for n in self.outputs:\n",
    "            gradients_from_loss_to_self = n.gradients[self] #输出节点对这个节点的偏导，self：指的是本身这个节点\n",
    "            self.gradients[self.x] += gradients_from_loss_to_self*self.partial()\n",
    "            \n",
    "\n",
    "class ReLu(Node):\n",
    "    def __init__(self,x,name=None,is_trainable = False):\n",
    "        Node.__init__(self,[x],name=name,is_trainable=is_trainable)\n",
    "        self.x = self.inputs[0]\n",
    "        \n",
    "    def forward(self):\n",
    "        self.value = self.x.value*(self.x.value > 0)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.gradients[self.x] = np.zeros_like(self.value)\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            gradients_from_loss_to_self = n.gradients[self]\n",
    "            self.gradients[self.x] += gradients_from_loss_to_self*(self.x.value > 0)\n",
    "            \n",
    "class MSE(Node):\n",
    "    \n",
    "    def __init__(self,y_pre,y,name,is_trainable=False):\n",
    "        \n",
    "        Node.__init__(self,[y_pre,y],name=name,is_trainable=is_trainable)\n",
    "        self.y_pre, self.y = self.inputs[0],self.inputs[1]\n",
    "        \n",
    "  \n",
    "    def forward(self):\n",
    "        y = self.y.value.reshape(-1,1)\n",
    "        y_pre = self.y_pre.value.reshape(-1,1)\n",
    "       \n",
    "        assert(y.shape == y_pre.shape)\n",
    "\n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - y_pre\n",
    "\n",
    "        self.value = np.mean(self.diff**2)\n",
    "        \n",
    "  \n",
    "    def backward(self):\n",
    "        \n",
    "        \n",
    "        self.gradients[self.y] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.y_pre] = (-2 / self.m) * self.diff  \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\"\"\"\n",
    "使用拓扑排序找到网络节点的前向计算顺序（反向传播反过来就行）\n",
    "\"\"\"\n",
    "def toplogical(graph):\n",
    "    \n",
    "    sorted_graph_nodes = []\n",
    "    \n",
    "    while graph: \n",
    "        all_nodes_have_inputs = set()\n",
    "        all_nodes_have_outputs = set()\n",
    "        \n",
    "        for have_output_node, have_inputs in graph.items():\n",
    "            all_nodes_have_outputs.add(have_output_node)#包括只有输出的节点 和既有输入又有输出的点\n",
    "            all_nodes_have_inputs |= set(have_inputs) #有输入的点：包括既有输入和输出的点 和只有输入的点（末尾终点）\n",
    "        need_removed_nodes = all_nodes_have_outputs - all_nodes_have_inputs #减去之后留下只有输出的节点\n",
    "        \n",
    "        if need_removed_nodes:\n",
    "            node = random.choice(list(need_removed_nodes))  #随机删去一个节点\n",
    "            visited_next = [node]\n",
    "            \n",
    "            if len(graph) == 1: visited_next += graph[node] #当最后删到只留一个有输出的节点\n",
    "                #的时候，那么需要把这个节点对应的输出节点也加上，否则会漏掉这个点\n",
    "                \n",
    "            graph.pop(node)\n",
    "            sorted_graph_nodes += visited_next\n",
    "            \n",
    "            for _, links in graph.items():\n",
    "                if node in links: links.remove(node) #如果删除的节点在别的节点的连接关系内，那么把他从连接关系里删除\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return sorted_graph_nodes\n",
    "    \n",
    "\"\"\"\n",
    "根据feed_dict和网络节点的初始化结果,建立网络的连接关系\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "def convert_feed_dict_graph(feed_dict):\n",
    "    computing_graph = defaultdict(list)\n",
    "    \n",
    "    nodes = [n for n in feed_dict]\n",
    "    \n",
    "    while nodes:\n",
    "        n = nodes.pop(0)  #移除列表中的一个元素（默认最后一个元素），并且返回该元素的值\n",
    "        \n",
    "    \n",
    "        if isinstance(n,Placeholder):\n",
    "            n.value = feed_dict[n]\n",
    "        if n in computing_graph:continue\n",
    "        \n",
    "        for m in n.outputs:\n",
    "            computing_graph[n].append(m) #建立好网络连接关系\n",
    "            nodes.append(m)\n",
    "            \n",
    "    return computing_graph\n",
    "   \n",
    "\"\"\"\n",
    "根据网络的连接关系，进行拓扑排序。\n",
    "\"\"\"\n",
    "def toplogical_sort(feed_dict):\n",
    "    \n",
    "    graph = convert_feed_dict_graph(feed_dict)\n",
    "    \n",
    "    return toplogical(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正向传播\n",
    "def forward(graph,monitor=False,train = True):\n",
    "    for node in graph if train else graph[:-1]:\n",
    "        if monitor:print('forward:{}'.format(node))\n",
    "        node.forward()\n",
    "#反向传播\n",
    "def backward(graph,monitor=False):\n",
    "    for node in graph[::-1]:\n",
    "        if monitor:print('backward:{}'.format(node))\n",
    "        node.backward() \n",
    "\"\"\"\n",
    "进行前向和反向传播计算\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "进行前向和反向传播计算\n",
    "\"\"\"\n",
    "def run_steps(graph_topological_sort_order,monitor=False,train=True):\n",
    "    if train:\n",
    "        forward(graph_topological_sort_order,monitor)\n",
    "        backward(graph_topological_sort_order,monitor)\n",
    "    else:\n",
    "        forward(graph_topological_sort_order,monitor,train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(graph,learning_rate=1e-2):\n",
    "    for node in graph:\n",
    "        if node.is_trainable:\n",
    "            node.value = node.value.reshape((node.value.shape[0],-1))\n",
    "            node.gradients[node] = node.gradients[node].reshape((node.gradients[node].shape[0],-1))\n",
    "            node.value += -1 * node.gradients[node] * learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,zipfile\n",
    "from glob import glob\n",
    "\n",
    "def compress(zip_file, input_dir):\n",
    "    f_zip = zipfile.ZipFile(zip_file, 'w')\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for f in files:\n",
    "             # 获取文件相对路径，在压缩包内建立相同的目录结构\n",
    "            abs_path = os.path.join(os.path.join(root, f))\n",
    "            rel_path = os.path.relpath(abs_path, os.path.dirname(input_dir))\n",
    "            f_zip.write(abs_path, rel_path, zipfile.ZIP_STORED)\n",
    "\n",
    "def extract(zip_file,pwd=None):\n",
    "    if pwd:\n",
    "        pwd = pwd.encode()\n",
    "    f_zip = zipfile.ZipFile(zip_file, 'r')\n",
    "    # 解压所有文件到指定目录\n",
    "    f_zip.extractall(zip_file.split(\".\")[0],pwd=pwd)\n",
    "\n",
    "    #return txt_file\n",
    "\n",
    "import shutil\n",
    "def save_model(save_path,model):\n",
    "\n",
    "    save_path = save_path.split('.')[0]\n",
    "    if not os.path.exists(save_path): #如果文件夹不存在，创建一个新的\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    for name, node in vars(model).items():\n",
    "        if isinstance(node, Placeholder):\n",
    "            if node.is_trainable:\n",
    "                np.savetxt(\"{}/{}.txt\".format(save_path,node.name), node.value)\n",
    "    compress(os.getcwd() + '/{}.zip'.format(save_path), save_path, )\n",
    "    shutil.rmtree(save_path)\n",
    "\n",
    "\n",
    "\n",
    "def load_model(load_path,model):\n",
    "\n",
    "    extract(load_path)\n",
    "    load_path = load_path.split(\".\")[0]\n",
    "    model_path = np.array(glob(load_path+\"/*/*\"))\n",
    "    for name, node in vars(model).items():\n",
    "        if isinstance(node, Placeholder):\n",
    "            if node.is_trainable:\n",
    "                for path in model_path:\n",
    "                    if path.split(\".\")[0].split(\"/\")[2] == node.name:\n",
    "                        node.value = np.loadtxt(path)\n",
    "    shutil.rmtree(load_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_sort [Node:b3, Node:w2, Node:w3, Node:y, Node:x, Node:b2, Node:w1, Node:b1, Node:linear1, Node:sigmoid, Node:linear2, Node:Relu, Node:linear3, Node:MSE]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle, resample\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#加载数据\n",
    "dataset = load_boston()\n",
    "\"\"\"\n",
    "print(dataset['feature_names'])\n",
    "print(dataset['data'].shape)\n",
    "print(dataset['target'].shape)\n",
    "\"\"\"\n",
    "x_ = dataset['data']\n",
    "y_ = dataset['target']\n",
    "\n",
    "# Normalize data\n",
    "x_ = (x_ - np.mean(x_, axis=0)) / np.std(x_, axis=0)\n",
    "\n",
    "\n",
    "# 定义网络\n",
    "class MLP():\n",
    "\n",
    "    def __init__(self, x_, y_):\n",
    "\n",
    "        self.x, self.y = Placeholder(\n",
    "            name='x', is_trainable=False), Placeholder(name='y',\n",
    "                                                       is_trainable=False)\n",
    "        self.w1, self.b1 = Placeholder(name='w1'), Placeholder(name='b1')\n",
    "        self.w2, self.b2 = Placeholder(name='w2'), Placeholder(name='b2')\n",
    "        self.w3, self.b3 = Placeholder(name='w3'), Placeholder(name='b3')\n",
    "\n",
    "        self.output1 = Linear(self.x, self.w1, self.b1, name='linear1')\n",
    "        self.output2 = Sigmoid(self.output1, name='sigmoid')\n",
    "        self.output3 = Linear(self.output2, self.w2, self.b2, name='linear2')\n",
    "        self.output4 = ReLu(self.output3, name='Relu')\n",
    "        self.y_pre = Linear(self.output4, self.w3, self.b3, name='linear3')\n",
    "        self.MSE_loss = MSE(self.y_pre, self.y, name='MSE')\n",
    "\n",
    "        hidden = 10\n",
    "        hidden1 = 16\n",
    "        output = 1\n",
    "        #初始化数据结构\n",
    "        self.feed_dict = {\n",
    "            self.x: x_,\n",
    "            self.y: y_,\n",
    "            self.w1: np.random.rand(x_.shape[1], hidden),\n",
    "            self.b1: np.zeros(hidden),\n",
    "            self.w2: np.random.rand(hidden, hidden1),\n",
    "            self.b2: np.zeros(hidden1),\n",
    "            self.w3: np.random.rand(hidden1, output),\n",
    "            self.b3: np.zeros(output),\n",
    "        }\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "mlp = MLP(x_, y_)\n",
    "graph_sort = toplogical_sort(mlp.feed_dict)  #拓扑排序\n",
    "print(\"graph_sort\", graph_sort)\n",
    "m = x_.shape[0]\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "\n",
    "def train(model,\n",
    "          epoch=5000,\n",
    "          learning_rate=1e-3,\n",
    "          steps_per_epoch=steps_per_epoch):\n",
    "    #开始训练\n",
    "    losses = []\n",
    "    for e in range(epoch):\n",
    "        loss = 0\n",
    "        for b in range(steps_per_epoch):\n",
    "            X_batch, y_batch = resample(x_, y_, n_samples=batch_size)\n",
    "            mlp.x.value = X_batch  # 在这更新值\n",
    "            mlp.y.value = y_batch\n",
    "            run_steps(graph_sort, monitor=False)\n",
    "\n",
    "            optimize(graph_sort, learning_rate=learning_rate)\n",
    "\n",
    "            loss += mlp.MSE_loss.value\n",
    "\n",
    "        print(\"epoch:{},loss:{}\".format(e, loss / steps_per_epoch))\n",
    "        losses.append(loss / steps_per_epoch)\n",
    "    #print(\"loss:{}\".format(np.mean(losses)))\n",
    "\n",
    "    save_model(\"mlp.zip\", model)\n",
    "    plt.plot(losses)\n",
    "    plt.savefig(\"many_vectoy.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测值： 17.6478606386398 真实值： 15.0\n"
     ]
    }
   ],
   "source": [
    "load_model(\"mlp.zip\",mlp)\n",
    "def predict(x_rm, graph,model):\n",
    "    model.x.value = x_rm\n",
    "    run_steps(graph, monitor=False, train=False)\n",
    "\n",
    "    return model.y_pre.value\n",
    "\n",
    "print(\"预测值：\",predict(x_[10],graph_sort,mlp),\"真实值：\",y_[10])\n",
    "    #预测值： 18.52534730151933 真实值： 15.0\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "py36torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f11ec32d488b58821b07bca992f76bf553f91446d669cc413baf1c802561df6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
